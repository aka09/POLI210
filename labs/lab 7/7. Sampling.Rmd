---
title: '7. Sampling'
date: 'POLI210, Week 9, Fall 2021'
output: pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      root.dir = rprojroot::find_rstudio_root_file())
```

We saw in class that polls are powerful tools to learn about public opinion and attitudes -- but only under certain conditions. Specifically, even small samples can provide very reliable estimates of population parameters^[Population parameters: A characteristic of the population that we would like to learn about and that we are trying to estimate using a sample (e.g. the mean height in the population).] if they are *representative* of the broader population. In this lab, we will take samples from our class survey. We will begin by importing our class survey.

```{r}
# Your path will of course be different
setwd("C:/Users/olivi/Documents/Grad/McGill Fall 2021/POLI210/labs/lab 7")

# Importing the class survey
survey <- read.csv("class_survey.csv")
```

The first function we want to be familiar with is `sample()`. This function takes three arguments: the first is a vector to sample from; the second is the number of elements to randomly draw from this vector; and the third is a TRUE/FALSE variable that indicates whether any given element can be drawn more than once. 

```{r}
# We give the function the vector of numbers from 1 to 10
# and ask the function to draw one random element 
sample(1:10, size = 1)

# sampling 10 random numbers from 1 to 10 (w/ replacement)
# therefore, any given number can appear more than once
sample(1:10, size = 10, replace = TRUE)
```

To make things more replicable, we can use the `set.seed()` function. It accepts a single argument: a number -- whatever it is. If you and I use `set.seed` with the same number as argument, we will get the same random numbers drawn. And, if I re-run the `sample()` function over and over again, I will always get the same result. Note that the numbers drawn are still random! 

```{r}
# the number in parentheses does not matter; one convention is to use 
# the date. 
set.seed(1024)

# I rerun the sample function from above. Now we should all get the same output
sample(1:10, size = 10, replace = TRUE)
```

As Olivier briefly showed in class, we can use the sample function in conjunction with indexing to draw random observations from our dataset. 

```{r}
# Remember how indexing works: anything before the comma indexes rows
# And anything after the comma indexes columns
# (nothing after the comma keeps all columns)
# So this extracts the first observation
survey[1,]
```

Instead of using a number of our own choosing inside the square brackets, we will use the numbers that are randomly generated by the `sample()` function.

```{r}
# We will store the number of observations in an object
nobs <- nrow(survey)

# We give the sample function a vector from 1 to the number of observations (285)
# It will draw one random number from 1 to 285
random_obs <- sample(1:nobs, size = 1, replace = TRUE)
random_obs
```
Now that we have this random number from 1 to 285, we can simply plug it into the square brackets to extract it from the survey. Remember that our number is stored in the `random_obs` object, so we can use this object inside the square brackets. 

```{r}
# This extracts one random observation from the survey 
survey[random_obs,]
```

## Exercise 

Before going any further, try to use the logic shown above to extract 10 random observations from the class survey. Store the 10 random observations in a new object. Take the mean ideology in this sample; how does it compare to the mean ideology in the entire survey? 

\newpage 

### Answer 

```{r}
# I use set.seed to get the same results every time
# otherwise, every time I knit I'll get a different result!
set.seed(1024)

# 10 random numbers from 1 to 285
random_10 <- sample(1:nobs, size = 10, replace = FALSE)

# Indexing using the vector of 10 numbers; store in new object
survey_10random <- survey[random_10,]

survey_10random

mean(survey_10random$ideology, na.rm = T)
mean(survey$ideology, na.rm = T)
```

The mean ideology in my 10-student sample is exactly 3. That's pretty close to the "true population parameter", which is roughly 2.95. What happens if we draw a different random sample? 

```{r}
set.seed(1033)

# 10 random numbers from 1 to 285
random_10_b <- sample(1:nobs, size = 10, replace = FALSE)

# Indexing using the vector of 10 numbers; store in new object
survey_10random_b <- survey[random_10_b,]

survey_10random_b

mean(survey_10random_b$ideology, na.rm = T)
mean(survey$ideology, na.rm = T)
```

Hmm, this sample is a little bit weirder! It estimates that the class is much more left-leaning than it actually is. But even with random sampling, this can happen -- it's part of **sampling variation**. If we draw 1,000 random samples from our class survey, the samples will, on average, have a mean ideology that is very close the true mean ideology in the full survey. However, some will a little bit off; and a few (but not many!) will be off by a lot. This is the central limit theorem: the sampling distribution (the distribution of means from many samples) will resemble a normal distribution and it will be centered on the true population parameter. 

If I draw many different samples, this is what the sampling distribution looks like...No need to understand this piece of code; just know that it does the exact same thing we did above, but repeats it 1,000 times. So, we take a random sample of 10 from the survey and compute the mean ideology in that sample; rinse and repeat, 1,000 times. The histogram below shows the distribution of sample means for those 1,000 samples.

```{r}
set.seed(123)
sample_means <- c()
for(i in 1:1000){
  sample_means[i] <- mean(survey$ideology[sample(1:nobs, 10)], na.rm = T)
}

hist(sample_means, breaks = seq(0, 6, 0.2))
abline(v = mean(sample_means), col = "red")
abline(v = mean(survey$ideology, na.rm = T), col = "blue")
```
In red is the mean from the 1,000 sample means. In blue is the true population -- i.e. the mean ideology in the full dataset. They're really, really close! That's exactly what we expect given random sampling. But note that certain samples are further off from the true parameter. Note also the shape of the distribution. Essentially, what this says is:

**On average, a random sample will be right on the true population parameter; sometimes, your random sample will be off, but it's more likely to be off by a bit than off by a lot.**

# Non-response bias

As we saw in class, non-response bias is a problem for surveys -- even if we sample randomly. If someone is randomly selected to be part of our sample and refuses, the people who actually respond to the survey will not be representative of the broader population. 

We will be simulating this sort of non-response bias. For each respondent, we will have a variable called `answer_dummy` that takes on the value of 1 if the respondent agrees to answer if contacted, and 0 otherwise. To do this, we will use the `rbinom()` function.

`rbinom` is a function that takes a probability --  a number between 0 and 1 -- and, based on that probability, randomly draws the number 0 and 1. The lower the probability that you specify as an argument to the function, the less likely you are to get a 1. The higher the probability, the more likely you are to get a 1. 

The `rbinom` function takes three argument, but we will ignore the middle one and simply give it the value 1 every time. The first argument is the number of draws that you want. The last argument is the probability for each draw.

```{r}
set.seed(1010)
# I am drawing one number that will either be 0 or 1, with a 99% 
# chance that it will be 1.
rbinom(n = 1, size = 1, prob = 0.99)

# I am drawing two numbers that will either be 0 or 1, with a 99% 
# chance that it will be 1.
rbinom(n = 2, size = 1, prob = 0.99)
```
Unsurprisingly, we get a draw of 1 every time here. This is not surprising: we set the probability to be 0.99! Let's try something else...

```{r}
set.seed(1010)
rbinom(n = 10, size = 1, prob = 0.2)
```
We drew 10 numbers, each of them having a 0.2 probability (a "20% chance") of being a 1 (and thus a 0.8 probability of being a 0). Looks like we got three 1s; not *exactly* what we expect given the probability that we specified, but hey -- this is a random process, there is bound to be some variation.

So what we'll do is that we'll give our male survey respondents a lower probability of responding to the survey. Here is the code for that

```{r}
set.seed(1027)

# ifelse function has three arguments: a condition, a behavior if the
# condition is true, and a behavior if the condition is false
# I assign this to a new variable, so that each respondent has an
# assigned probability of answering the survey
# So all men have a 50% chance of answering, while all women have a 
# 75% chance of answering
survey$prob_answer <- ifelse(survey$gender == "Male", 0.5, 0.75)

# I draw 285 0s and 1s; for each draw, the probability that it is a 1
# is determined by the value of the new variable I created just above
survey$answer_dummy <- rbinom(n = nrow(survey), 1, prob = survey$prob_answer)

# What does this look like? 
head(survey)

# We will repeat what we did before: draw ten random numbers from 
# 1 to 285, and subset the dataframe using these random numbers
random_10_c <- sample(1:nobs, size = 10, replace = FALSE)

# Here I have my 10-student sample 
survey_10random_c <- survey[random_10_c,]

# This is what it looks like
# Some people here were sampled, meaning they were selected to be 
# part of the survey, but they refused! 
survey_10random_c

# Let's only keep those who were sampled AND agreed to participate
survey_10random_c <- survey_10random_c[survey_10random_c$answer_dummy==1,]

# And take the mean ideology 
mean(survey_10random_c$ideology, na.rm = T)
```
Hmmm, that's quite a bit off. But maybe I was just unlucky and suffered from sampling variation? Well, we know that's not the case: the men are less likely to agree to participate if they are contacted. That's the way I set up the data! How big of a problem is this? 

If you remember from the first problem set, men tend to be more right-leaning (or, at least, less left-leaning) than women in the class. So, this **non-response bias** will bias our estimate of ideology *downward*. Meaning, on average, the samples will estimate that the class is more left-leaning than it actually is. 

In the code below (which you do not need to understand), I draw 1,000 samples and take into account non-response. You still have the true population parameter in blue. But now, the red line is not close to the blue line -- meaning, our samples are, on average, biased downward. 

```{r}
set.seed(123)
sample_means_biased <- c()
for(i in 1:1000){
  contacted <- survey[sample(1:nobs, 10),]
  responded <- contacted[contacted$answer_dummy==1,]
  sample_means_biased[i] <- mean(responded$ideology, na.rm = T)
}

hist(sample_means_biased, breaks = seq(0, 6, 0.2))
abline(v = mean(sample_means_biased), col = "red")
abline(v = mean(survey$ideology, na.rm = T), col = "blue")
```

If men were even less likely to answer the survey, the magnitude of the bias would be larger. Similarly, if the difference in ideology between men and women were larger, the magnitude of bias would also be larger. In short, sample composition matters: the people who refuse to take part in a survey are probably different in some way, and this can bias our estimation of population parameters. Sample statistics are no longer a good guide to guess what the population parameter is. 

## Playing around with a sampling distribution

If you have still have time, navigate to [the following link] and play around with the sampling distribution of ideology. (https://3nzgux-olivier-bergeron0boutin.shinyapps.io/shiny_sampling/). 





